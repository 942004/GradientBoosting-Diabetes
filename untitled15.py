# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12erf_6K_vL3wNdg7zvLXVeiGQWpG_gY2
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Load the dataset
data = load_diabetes()
X, y = data.data, data.target

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define different weak learners
depths = [1, 3, 7]  # Stumps, 10-node trees, 100-node trees
colors = ['blue', 'red', 'green']
n_estimators = 200

plt.figure(figsize=(10, 6))

for depth, color in zip(depths, colors):
    test_mse = []
    model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=0.1, max_depth=depth, random_state=42)
    for i in range(1, n_estimators + 1):
        model.n_estimators = i
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        test_mse.append(mean_squared_error(y_test, y_pred))

    plt.plot(range(1, n_estimators + 1), test_mse, label=f'max_depth={depth}', color=color)

# Plot settings
plt.xlabel('Number of Estimators')
plt.ylabel('Test MSE')
plt.title('Gradient Boosting Performance with Different Weak Learners')
plt.legend()
plt.show()